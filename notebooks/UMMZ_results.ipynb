{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMMZ Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "from random import sample\n",
    "import json\n",
    "import jsonlines\n",
    "import pysbd\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False) # sentence segmenter\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ner_model = spacy.load(\"../../specimen-ner/output/model-best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2txt_df(json_file_path):\n",
    "    \"\"\"\n",
    "    Extract body text of json file\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "        body_text_df = pd.json_normalize(data[\"pdf_parse\"][\"body_text\"])\n",
    "    \n",
    "    return body_text_df\n",
    "\n",
    "def df2sentences(df):\n",
    "    \"\"\"\n",
    "    Tokenize and clean sentences\n",
    "    \"\"\"\n",
    "    sentences_text = \" \".join(list(df.text)) \n",
    "    pattern_brackets = re.compile(r'\\(.*?\\)')\n",
    "    sentences_text = re.sub(pattern_brackets, \"\", sentences_text) \n",
    "    sentences = seg.segment(sentences_text) \n",
    "    sentences = [re.sub(r\"^\\W+\", \"\", sentence) for sentence in sentences] \n",
    "    sentences = [re.sub(r\"\\s+\", \" \", sentence) for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "def prep_data(json_file_path, ref_id):\n",
    "    \"\"\"\n",
    "    Return sentence lists for matches\n",
    "    \"\"\"\n",
    "    body_text_df = json2txt_df(json_file_path)\n",
    "    sentences = df2sentences(body_text_df)\n",
    "    candidates = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        matches = matcher(doc)\n",
    "        if len(matches) > 0:\n",
    "            sentence_dict = {\"text\":sentence,\n",
    "                             \"meta\":{\"ref_id\":str(ref_id)}}\n",
    "            candidates.append(sentence_dict)    \n",
    "    return candidates\n",
    "\n",
    "def clean_text(txt):\n",
    "    \"\"\"\n",
    "    Removes special characters, punctuation, and returns lowercase\n",
    "    \"\"\"\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
    "\n",
    "def clean_text2(txt):\n",
    "    \"\"\"\n",
    "    Removes special characters and punctuation\n",
    "    \"\"\"\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt))\n",
    "\n",
    "def alphanum(txt):\n",
    "    \"\"\"\n",
    "    Extracts alphanumeric characters without spaces\n",
    "    \"\"\"\n",
    "    return \"\".join(x.lower() for x in txt if x.isalnum())\n",
    "\n",
    "def extract_pat(search_str:str, search_list:str):\n",
    "    \"\"\"\n",
    "    Defines pattern to search for\n",
    "    \"\"\"\n",
    "    search_obj = re.search(search_list, search_str)\n",
    "    if search_obj:\n",
    "        return_str = search_str[search_obj.start(): search_obj.end()]\n",
    "    else:\n",
    "        return_str = \"None\"\n",
    "    return return_str\n",
    "\n",
    "def extract_ents(text):\n",
    "    \"\"\"\n",
    "    Extract named entities, and beginning, middle and end idx     \n",
    "    \"\"\"\n",
    "    doc = custom_ner_model(text)\n",
    "    if len(doc.ents) > 0:\n",
    "        return clean_text2(doc.ents)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def predict_ents(df):\n",
    "    \"\"\"\n",
    "    Create new column in data frame with named entity tuple extracted\n",
    "    \"\"\"\n",
    "    df_sent['specimen_prediction'] = df_sent['sent'].apply(extract_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for SPECIMEN entities in 461 UMMZ papers\n"
     ]
    }
   ],
   "source": [
    "ummz_papers = glob.glob('/nfs/turbo/isr-slafia/specimen/bibliography_ummz_json/*.json')\n",
    "print(f\"Looking for SPECIMEN entities in {len(ummz_papers)} UMMZ papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for SPECIMEN entities in paper: Hooper1944\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Number 87338\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SPECIMEN</span>\n",
       "</mark>\n",
       " was collected near Eagle Creek on the Madison Ranch about six miles north northwest of Warrick in the Bearpaw Mountains of northern Montana </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 s, sys: 971 Âµs, total: 1.67 s\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "papers_sample = random.sample(ummz_papers, 1)\n",
    "\n",
    "for file in papers_sample:\n",
    "    ref_id = file.split(\"/\")[-1].split(\"-\")[0]\n",
    "    print(f\"Searching for SPECIMEN entities in paper: {ref_id}\")\n",
    "    body_text_df = json2txt_df(file)\n",
    "    sentence_list = df2sentences(body_text_df)\n",
    "    for sentence in sentence_list:\n",
    "        clean_sent = clean_text2(sentence)\n",
    "        doc = custom_ner_model(clean_sent)\n",
    "        if len(doc.ents) > 0:\n",
    "            displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_test_pubs = pd.DataFrame()\n",
    "for test_file in ummz_papers:\n",
    "    with open(test_file, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "        file_data = pd.json_normalize(data[\"pdf_parse\"][\"body_text\"])\n",
    "        file_data[\"paper_id\"] = data[\"title\"]\n",
    "    df_test_pubs = pd.concat([df_test_pubs, file_data])\n",
    "\n",
    "ummz_sentences = []\n",
    "\n",
    "for row in df_test_pubs.itertuples():\n",
    "    for sent in seg.segment(row[1]):\n",
    "        ummz_sentences.append((row[7], row[5], row[7], sent))\n",
    "        \n",
    "df_sent = pd.DataFrame(ummz_sentences, columns=['Id', 'section_title', 'paper_title', 'sent'])\n",
    "df_sent['sent'] = df_sent['sent'].astype(str)\n",
    "df_sent\n",
    "    \n",
    "predict_ents(df_sent)\n",
    "\n",
    "df_sent['specimen_prediction'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df_sent['specimen_prediction'].value_counts().nlargest(50).sort_values().plot(kind='barh',figsize=(20,10))\n",
    "plt.savefig('../../specimen-ner/results/ummz_specimen_count.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ummz_unique_codes = set(df_sent['specimen_prediction'].unique())\n",
    "len(ummz_unique_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ummz = df_sent[df_sent.specimen_prediction.notna()]\n",
    "predictions_ummz = predictions_ummz[predictions_ummz.Id.notna()]\n",
    "predictions_ummz['paper'] = predictions_ummz['Id'].astype(\"category\").cat.codes\n",
    "\n",
    "predictions_ummz['Id'] = predictions_ummz[['Id','paper','specimen_prediction']].groupby(['specimen_prediction','paper'])['Id'].transform(lambda x: ','.join(x))\n",
    "predictions_ummz.to_csv('../../specimen-ner/results/predictions_ummz.csv',index=False)\n",
    "\n",
    "predictions_ummz_dedup = predictions_ummz[['specimen_prediction','Id','paper']].drop_duplicates()\n",
    "\n",
    "cross_df = pd.crosstab(predictions_ummz_dedup.specimen_prediction, predictions_ummz_dedup.paper, margins=True, dropna=False)\n",
    "cross_df.to_csv('../../specimen-ner/results/frequency_ummz.csv')\n",
    "cross_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
